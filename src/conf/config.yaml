# defaults
defaults:
  - model: bandsplitrnnV7
  - train_dataset: default
  - val_dataset: default # comment out to ignore validation data (i.e. for final training)
  - test_dataset: default
  - sad: default
  - augmentations: default
  - featurizer: stft
  - callbacks: default
  - logger: ['tensorboard','wandb'] # set logger here or use command line (e.g. python run.py logger=['tensorboard','wandb'])
  - _self_

# data
train_loader:
  batch_size: 8 # increase for faster training, reduce for less memory usage
  num_workers: 12 # set to same number as CPU cores
  shuffle: True
  drop_last: True
val_loader:
  batch_size: 8 # increase for faster training, reduce for less memory usage
  num_workers: 12 # set to same number as CPU cores 
  shuffle: False
  drop_last: False

# Optimization
opt:
  _target_: torch.optim.Adam
  lr: 1e-3

# Scheduler options:
# StepLR (torch.optim.lr_scheduler.StepLR)
# sch:
#   _target_: torch.optim.lr_scheduler.StepLR
#   step_size: 2
#   gamma: 0.98

# LambdaLR (torch.optim.lr_scheduler.LambdaLR):
# allows for the incorporation of a warmup step
sch:
    warmup_step: 10
    alpha: 0.1 # warmup alpha for exponential warmup (to the power of the warmup step), originally 0.1
    gamma: 0.9899494936611665 # (aka sqrt(0.98) -> the same as decaying 0.98 every 2 epochs rather than 1, per the paper)

ckpt_path: null # use this argument to begin training at a checkpoint
# enable_validation: False # True/False to enable validation

trainer:
  fast_dev_run: False
  min_epochs: 100 # optional (included early stopping callback)
  max_epochs: 500
  val_check_interval: 1 # comment out to ignore validation data (i.e. for final training)
  check_val_every_n_epoch: 1 # comment out to ignore validation data (i.e. for final training)
  num_sanity_val_steps: 5 # comment out to ignore validation data (i.e. for final training)
  log_every_n_steps: 10 # 100
  accelerator: auto # "auto", gpu
  devices: auto # "auto" or # - can increase devices with more gpus available
  gradient_clip_val: 5 # gradient clipping by a maximum gradient norm of 5 per 2022 paper
  precision: 32 # 16 for lower memory load, 32 for better accuracy
  enable_progress_bar: True
  benchmark: True
  deterministic: False

# hydra
experiment_dirname: bandsplitrnn
hydra:
  run:
    dir: logs/${...experiment_dirname}/${now:%Y-%m-%d}_${now:%H-%M}
  job:
    chdir: False

torch:
  matmul_precision: highest

# wandb
wandb_api_key: ${oc.env:WANDB_API_KEY}